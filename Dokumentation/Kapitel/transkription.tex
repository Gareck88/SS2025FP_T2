\chapter{Transkription und Sprecherdiarisierung}
\label{chap:ki}
\authormargin{Fabian Scherer}

Die automatische Spracherkennung (ASR) und die Sprecherdiarisierung (SD) sind zentrale Komponenten dieses Projekts, die es ermöglichen, kontinuierliche Audiodaten in eine textbasierte und sprecherbasierte Ausgabe umzuwandeln. Beide Funktionen laufen in einem separaten Python-Backend, das in Echtzeit mit dem C++-Frontend über Standard-Pipes kommuniziert.


\section{Audiodaten-Pipeline}
\authormargin{Fabian Scherer}

Die Audiodaten werden im C++-Frontend erfasst und als normalisierte 32-Bit-Float-Werte im Bereich von -1.0 bis 1.0 verarbeitet. Diese werden kontinuierlich in binärer Form an das Python-Backend gesendet.

Auf der Python-Seite werden die empfangenen Byte-Daten chunkweise ausgelesen, wobei ein Chunk einer Sekunde entspricht, in NumPy-Arrays umgewandelt und an die jeweiligen Verarbeitungsmodule (ASR und SD) weitergeleitet.


\section{Automatische Spracherkennung (ASR)}
\authormargin{Fabian Scherer}

Für die ASR-Funktionalität wird ein vortrainiertes \textbf{Wav2Vec2-Modell} verwendet, das für die deutsche Sprache optimiert ist. Das Modell ist auf eine Samplerate von 16 kHz ausgelegt, weshalb die eingehenden Audiodaten auf diese Rate resampelt werden, um eine hohe Genauigkeit zu gewährleisten.

Der Verarbeitungsprozess gliedert sich wie folgt:
\begin{itemize}
\item \textbf{1. Input-Verarbeitung:} Jeder 1-Sekunden-Audio-Chunk wird direkt in einen Tensoren umgewandelt.
\item \textbf{2. Inferenz:} Der Tensor wird durch das Wav2Vec2-Modell geleitet, welches eine Sequenz von Logits generiert.
\item \textbf{3. Dekodierung:} Ein \textbf{Language Model (KenLM)} wird verwendet, um die Logits in die wahrscheinlichste Wortsequenz zu dekodieren. Dieses Modell verbessert die Transkriptionsqualität erheblich, indem es den Kontext und die grammatikalische Struktur der Sprache berücksichtigt.
\item \textbf{4. Ausgabe:} Der transkribierte Text wird als JSON-Nachricht zurück an das C++-Frontend gesendet.
\end{itemize}

\subsection{Das verwendete Modell}

Die automatische Spracherkennung (ASR) in diesem Projekt basiert auf dem Wav2Vec2-Modell, insbesondere der für das Deutsche angepassten Variante aware-ai/wav2vec2-large-xlsr-53-german-with-lm (aware-ai, n.d.). Wav2Vec2 stellt einen signifikanten Fortschritt in der ASR-Forschung dar, indem es die Notwendigkeit großer, manuell transkribierter Korpora für das Training reduziert. Die Architektur zeichnet sich durch einen zweistufigen Lernprozess aus: selbstüberwachtes Vor-Training auf unbeschrifteten Audiodaten und überwachtes Fein-Tuning für eine spezifische Transkriptionsaufgabe (Baevski et al., 2020).

\subsubsection{Vor-Training (Pre-training)}

Die Grundlage des verwendeten Modells bildet die Wav2Vec2-XLS-R-53-Architektur. Im Vor-Trainingsschritt lernt das Modell die intrinsische Struktur und linguistische Eigenheiten von Sprache durch ein selbstüberwachtes Lernverfahren. Hierbei werden große Mengen unbeschrifteter Audiodaten als Input verwendet. Der Prozess kann wie folgt beschrieben werden: Das Modell maskiert (verdeckt) Teile des Audio-Eingangs und lernt anschließend, diese maskierten Segmente aus dem umliegenden Kontext vorherzusagen. Dieser Ansatz ermöglicht es dem Modell, robuste und kontextreiche Repräsentationen von Sprachlauten zu extrahieren, ohne semantisches oder linguistisches Wissen zu benötigen. Die Basisvariante wurde auf einem umfangreichen Datensatz von 436.000 Stunden unbeschrifteter Audiodaten in 53 verschiedenen Sprachen vor-trainiert (Baevski et al., 2020).

\subsubsection{Feinabstimmung (Fine-tuning)}

Nach dem Vor-Training wurde das Modell aware-ai/wav2vec2-large-xlsr-53-german-with-lm für die automatische Spracherkennung der deutschen Sprache feinabgestimmt. In dieser Phase wird der vortrainierte Feature-Extraktor mit einem spezifischen Klassifikator verbunden, um Audiodaten direkt in Text zu transkribieren. Ein entscheidendes Merkmal dieses Modells ist die Integration eines KenLM-Sprachmodells (Language Model). Dieses LM wird zur Dekodierung der Modellausgaben verwendet, um die Wahrscheinlichkeit von Wortsequenzen zu bewerten und so die grammatikalisch und semantisch plausibelste Transkription zu ermitteln. Die Kombination eines leistungsstarken, vor-trainierten Modells mit einem sprachspezifischen LM ermöglicht eine hohe Transkriptionsgenauigkeit und Robustheit gegenüber akustischen und linguistischen Variationen.

\section{Sprecherdiarisierung (SD)}
\authormargin{Fabian Scherer}

Die Sprecherdiarisierung hat die Aufgabe, zu erkennen, wann welcher Sprecher spricht. Dies ist besonders nützlich bei Gesprächen mit mehreren Teilnehmern.

Im Gegensatz zur ASR, die in Echtzeit auf jedem 1-Sekunden-Chunk arbeitet, verarbeitet die Diarisierung Audio in längeren Segmenten. Ein separater Puffer sammelt drei aufeinanderfolgende 1-Sekunden-Chunks, um eine 3-Sekunden-Analyse durchzuführen.

Der Prozess umfasst:
\begin{itemize}
\item \textbf{1. Chunk-Akkumulation:} Drei 1-Sekunden-Audio-Chunks werden zu einem 3-Sekunden-Segment zusammengefügt.
\item \textbf{2. Sprechereinbettung:} Das Audiosignal wird durch ein vortrainiertes Modell (wie z.B. Pyannote.audio) geleitet, um eine kompakte Darstellung der Stimme des Sprechers (eine sogenannte "Sprechereinbettung") zu erzeugen.
\item \textbf{3. Clustering:} Die Sprechereinbettung wird analysiert und mit zuvor erkannten Sprechern verglichen. Das Modell weist dem Segment einen bestimmten Sprecher zu oder identifiziert einen neuen.
\item \textbf{4. Ausgabe:} Das Ergebnis der Diarisierung (z.B. "Sprecher A hat von Sekunde 0 bis 3 gesprochen") wird als JSON-Nachricht an das C++-Frontend übermittelt.
\end{itemize}